1. 프로젝트 목표
1.1. 과제의 핵심 정의
본 프로젝트의 가장 핵심적인 목표는 인공지능 에이전트가 스스로의 학습을 통해 불안정한 시스템의 균형을 제어하는 능력을 획득하는 것입니다.
사전에 프로그래밍된 어떠한 물리 공식이나 제어 로직 없이 인공지능이 시행착오를 거치며 얻는 경험 데이터에 의존하여 문제를 해결해야 함을 의미합니다.
인공지능은 백지상태에서 시작하여 어떤 행동이 긍정적이고 부정적인 결과를 낳는지를 스스로 판단하고, 이를 바탕으로 최적의 제어 전략을 점진적으로 구축해 나가야 합니다.

1.2. 구체적인 성공 기준 및 목표
본 프로젝트의 성공을 판단하기 위해 다음과 같은 구체적이고 측정 가능한 목표들을 설정했습니다.
1. 공을 원탁 밖으로 떨어뜨리지 않고 최대한 오랜 시간 동안 생존시키는 것입니다. 최종적으로 한 에피소드의 최대 길이인 1000스텝(약 32초) 동안 완벽하게 임무를 수행하는 것을 목표로 합니다.
2. 공이 정지 상태에 머무르지 않고 원탁 위를 굴러다니는 상태를 유지해야 합니다. 이는 AI가 소극적인 방어 제어에서 벗어나게 함을 의미합니다. 이 목표는 학습된 정책의 정교함과 예측 능력을 판단하는 중요한 기준이 될 것입니다.
3. 이산 행동 기반의 DQN과 연속 행동 기반의 SAC 중 어떤 알고리즘이 더 효과적으로 적응하고 학습하는지 비교 분석합니다. 이를 통해 각 알고리즘의 장단점을 명확히 파악하고 향후 더 복잡한 동적 제어 과제에 적합한 알고리즘을 선택할 수 있는 기술적 근거를 마련하는 것을 목표로 합니다.


2. 프로젝트 환경
2.1. 시뮬레이션 플랫폼
본 프로젝트의 모든 실험은 webots을 이용하였습니다. 웹봇은 실제 로봇을 제작하고 실험하기 전에 가상 환경에서 먼저 테스트해볼 수 있도록 만들어진 강력한 개발 도구입니다.
저희가 웹봇을 시뮬레이션 플랫폼으로 선정한 이유는 정확한 물리 엔진 탑재, 안전성 및 비용 효율성, 통제된 실험 환경, 파이썬 연동 지원 등 뛰어난 시뮬레이션 환경과 개발환경을 지원하기 때문입니다.

2.2. 가상 환경 구성 요소
원탁: 인공지능이 직접 제어해야 할 대상입니다. 웹봇에서는 cylinder 형태의 객체로 구현했으며 그 제원은 다음과 같습니다.
반지름: 1.0 미터
높이: 0.01 미터

공: 원탁 위에서 균형을 맞춰야 할 목표물입니다. 웹봇에서는 sphere 형태로 구현했습니다.
반지름: 0.075 미터

모터:
메커니즘: 원탁은 하부 지지대와 볼 조인트라는 부품으로 연결되어 있으며 이 조인트는 3개의 개별 회전 모터에 의해 제어됩니다.
제어 축 선택 및 이유: 3개의 모터는 각각 다른 회전축을 담당합니다.
모터 1: 좌우 기울기, 즉 roll을 담당합니다.
모터 2: 수평 회전, 즉 yaw를 담당합니다.
모터 3: 앞뒤 기울기, 즉 pitch를 담당합니다.
공의 균형을 제어하는 데 직접적으로 기여하는 기울기를 만드는 축은 roll과 pitch뿐입니다. 수평 회전인 yaw는 공의 위치에 영향을 주지 않으므로 의도적으로 제외했습니다.
제어에 사용하는 두 모터의 최대 회전 속도는 1.0, 목표 위치(각도)의 범위는 -1.57에서 +1.57 라디안 사이입니다.

센서:
목표 객체 위치 센서: 인공지능이 공의 위치를 알기 위해 웹봇의 슈퍼바이저 기능을 사용했습니다. 이를 통해 매 순간 공의 정확한 3차원 위치(x, y, z)를 얻어 인공지능의 입력값으로 사용했습니다.
자체 상태 센서: 인공지능은 원탁 자체의 현재 기울기 또한 알아야 합니다. 이를 위해 제어에 사용되는 모터 1과 모터 3에 각각 포지션 센서를 활용했습니다. 이 센서들은 현실 세계의 자이로스코프나 경사계처럼 작동하여, 현재 원탁의 roll 및 pitch 각도를 정밀하게 측정합니다.


3. 적용 알고리즘 설계 및 구현 상세
3.1.  DQN 상세 설계
신경망 구조
DQN의 두뇌 역할을 하는 신경망은 'DQNNetwork'라는 이름으로 구현되었으며 그 구조는 다음과 같습니다.

입력층: 6개의 뉴런으로 구성됩니다. 각 뉴런은 State 정보의 한 요소, 즉 [공의 현재 x좌표, 공의 현재 y좌표, 공의 직전 x좌표, 직전 y좌표, 원탁의 현재 roll 각도, 원탁의 현재 pitch 각도]를 각각 입력받습니다.
은닉층: 2개의 층으로 구성되며, 각 층은 128개의 뉴런을 가집니다. 은닉층 뒤에 ReLU 함수를 사용하여 신경망이 단순한 선형 관계를 넘어선 복잡한 문제 해결 능력을 갖도록 했습니다.
출력층: 49개의 뉴런으로 구성됩니다. 이 숫자는 49개의 이산적인 행동 선택지입니다. 각 뉴런은 49개의 행동 중 하나에 대한 예측 Q 값을 각각 출력합니다.

핵심 학습 메커니즘
DQN은 본질적으로 여러 선택지 중 하나를 고르는 문제에 특화되어 있습니다. 따라서 모터가 취할 수 있는 -90도에서 +90도에 이르는 roll과 pitch의 각도 범위를 각각 7개의 구간으로 나누어, 총 7 곱하기 7, 즉 49개의 고정된 행동 조합을 만들었습니다. 에이전트는 이 49개의 정해진 행동 중 하나만 선택할 수 있습니다.
학습 초기에 에이전트는 아무것도 모릅니다. 따라서 무작위로 행동하며 어떤 행동이 좋은지 탐험해야 하는데 엡실론-그리디는 바로 이 탐험을 위한 장치입니다. 확률값에 따라 에이전트는 자신이 아는 최선의 행동을 할지, 아니면 완전히 새로운 무작위 행동을 할지 결정합니다. 이 엡실론 값은 학습이 진행될수록 0.995의 비율로 점차 감소하여 최종적으로 0.05(5%)까지 낮아집니다.
강화학습에서는 정답이 계속 바뀌는 문제가 발생하여 학습이 불안정해질 수 있습니다. 이 문제를 해결하기 위해 저희는 학습에 사용하는 메인 신경망 외에 이와 똑같이 생긴 타겟 신경망이라는 복사본을 하나 더 만들었습니다. 타겟 신경망은 평소에는 업데이트되지 않고 100 스텝마다 메인 신경망의 상태를 그대로 복사해옵니다. 이렇게 고정된 타겟을 기준으로 학습을 진행함으로써 전체 학습 과정이 훨씬 안정적으로 이루어집니다.
경험을 순서대로 학습하면 데이터 간의 상관관계가 너무 높아 학습이 비효율적이고 불안정해집니다. 이를 해결하기 위해 리플레이 버퍼를 사용합니다. 이 문제에선 최대 100만 개의 경험을 저장할 수 있는 버퍼를 만들었습니다. 에이전트는 새로운 경험을 할 때마다 이곳에 저장해두고 학습 시에는 저장된 경험들 중 256개를 무작위로 꺼내어 학습합니다. 이를 통해 상관관계를 깨고, 다양한 과거 경험을 바탕으로 안정적이고 효율적인 학습을 진행할 수 있습니다.

3.2. SAC (소프트 액터-크리틱) 상세 설계
기본 개념 및 접근 방식
SAC는 '액터-크리틱'이라는 구조를 기반으로 하는 연속적인 행동 제어에 강력한 알고리즘입니다. 이름처럼 액터와 크리틱이라는 두 종류의 신경망이 서로 협력하여 문제를 해결합니다.
액터는 어떤 행동을 할지 결정하는 선수, 크리틱은 그 행동이 얼마나 좋은지 평가하고 조언하는 코치에 비유할 수 있습니다.

신경망 구조

액터 (정책 신경망):
입력층: DQN과 동일하게 6개의 상태 정보를 입력받습니다.
은닉층: 2개의 층으로 구성되며 각 층은 256개의 뉴런을 가집니다.
출력층: 특정 행동 하나를 출력하는 대신 행동의 확률 분포를 출력합니다. 구체적으로는 roll 및 pitch 제어에 대한 정규분포의 평균과 표준편차를 각각 출력합니다.
크리틱 (가치 신경망):
입력층: 총 8개의 뉴런으로 구성됩니다. 이는 6개의 상태 정보뿐만 아니라 액터가 결정한 2개의 action 정보(roll, pitch 값)까지 함께 입력받기 때문입니다. 크리틱은 상태와 행동을 쌍으로 묶어 평가합니다.(상황에서 상태와 행동을 동시에 평가)
은닉층: 액터와 마찬가지로 256개 뉴런을 가진 2개의 은닉층으로 구성됩니다.
출력층: 단 하나의 뉴런으로 구성되며 입력된 상태-행동 쌍에 대한 예측 Q 값을 출력합니다.

핵심 학습 메커니즘
SAC는 학습 안정성을 극대화하기 위해 크리틱 신경망을 2개 사용합니다. 두 크리틱이 각각 독립적으로 가치를 평가한 뒤, 보수적인 평가값을 기준으로 학습을 진행합니다. 이 장치를 통해 긍정적인 편향을 줄여 훨씬 안정적인 학습이 가능해집니다.
SAC의 특징인 최대 엔트로피 적용입니다. 학습의 목표는 보상 + 엔트로피 보너스를 최대로 받는 것입니다. 엔트로피는 행동의 불확실성을 의미합니다. 따라서 에이전트는 높은 보상을 추구하면서도 동시에 자신의 행동을 가능한 한 예측 불가능하게 유지하려는 경향을 갖게 됩니다. 저희는 이 엔트로피의 중요도를 조절하는 알파 값을 0.2로 설정했습니다.
SAC는 타겟 네트워크가 일정 주기로 업데이트되는 소프트 업데이트 방식을 사용합니다. 매 학습 스텝마다 타겟 신경망의 가중치를 메인 신경망 쪽으로 0.5퍼센트 씩 이동시킵니다. 타겟 값을 매우 부드럽고 안정적으로 갱신하여 전체 학습 과정의 안정성을 크게 향상시킵니다.


4. 학습 설계
4.1. 학습의 기본 단위
인공지능의 학습은 에피소드라는 독립적인 시도들의 반복으로 이루어집니다. 각 에피소드는 여러 개의 스텝으로 구성됩니다.
시작: 에피소드가 시작되면 시뮬레이션 환경은 항상 동일한 상태로 초기화됩니다. 원탁은 수평, 공은 (0, 0, 0.1)의 좌표로 이동합니다.
종료 조건 1: 에이전트가 제어에 실패하여 공이 원탁 밖으로 떨어지면 에피소드는 즉시 종료됩니다.
종료 조건 2: 에이전트가 제어에 성공하여 공을 떨어뜨리지 않고 최대 설정된 시간 동안 버텨냈을 경우에도 에피소드는 성공적으로 종료됩니다. 한 에피소드의 최대 길이를 1000 스텝으로 설정했으며 약 32초에 해당합니다.

4.2. State
저희는 에이전트가 상황을 종합적으로 판단할 수 있도록 다음 6가지의 구체적인 수치 값을 상태 정보로 정의하여 제공했습니다.
공의 현재 위치 (x좌표, y좌표): 현재 공이 원탁의 중심으로부터 얼마나 떨어져 있는지를 나타냅니다.
공의 직전 위치 (직전 x좌표, 직전 y좌표): 바로 이전 행동 주기의 공의 위치입니다. (공의 벡터 값을 표현할 수 있을텐데 찾을 수가 없었습니다.)
원탁의 현재 기울기 (roll 각도, pitch 각도): 현재 원탁의 좌우 및 앞뒤 기울기 각도입니다.

4.3. action
DQN의 이산적 행동:
DQN은 여러 개의 정해진 선택지 중 하나를 고르는 방식으로 작동합니다. -1.57에서 +1.57 제어 범위를 7개의 대표 구간으로 균등하게 나누었습니다. 7(roll) * 7(pitch) = 49개의 고유한 행동 조합을 만들었습니다.

SAC의 연속적 행동:
SAC는 액터-크리틱 구조 덕분에 연속적인 행동 공간을 직접 다룰 수 있습니다. SAC의 액터 신경망은 특정 행동을 직접 선택하는 대신 행동의 확률 분포를 출력합니다.

4.4. Reward
공의 이동 크기에 따라 최소 0.5에서 최대 2.0 사이로 리워드를 부여합니다.
에피소드가 끝나지 않고 버티는 매 행동 주기마다 작은 양의 리워드를 지속적으로 제공합니다. (0.02 * 액션 주기(스탭 스킵 인터벌?))
공이 원탁 밖으로 떨어지면 -10.0의 리워드를 부여합니다.
에피소드의 최대 스텝(1000)을 모두 성공적으로 채웠을 때 +30.0의 리워드를 부여합니다.
공이 이전 스텝과 비교하여 전혀 움직이지 않았을 때 -0.1이라는 리워드를 부여합니다.


5. 학습 결과
5.1. 학습 과정에 대한 정량적 분석
각 알고리즘에 대해 세 종류의 그래프가 생성되었습니다.

그래프 종류 설명
에피소드 vs 총 보상: 에이전트가 한 번의 에피소드에서 얼마나 좋은 성과를 냈는지 보여주는 가장 직접적인 지표입니다. 그래프가 안정적으로 우상향할수록 학습이 잘 되고 있음을 의미합니다.
에피소드 vs 생존 스텝: 에이전트가 공을 떨어뜨리지 않고 얼마나 오래 버텼는지를 보여줍니다.
총 보상 vs 생존 스텝: 보상과 생존 스텝 두 지표가 어떤 관계를 보이는지 분석하기 위한 산점도 그래프입니다.

DQN 학습 과정 분석
그래프에서 가장 눈에 띄는 특징은 보상과 생존 스텝 그래프가 매우 큰 폭으로 위아래로 진동한다는 점입니다. 이는 학습이 안정적으로 수렴하지 못했음을 의미합니다.
에이전트는 때때로 300스텝 이상 생존하며 높은 보상(최대 약 80점)을 기록하는 성공적인 에피소드를 만들어내지만 바로 다음 에피소드에서는 100스텝도 채 버티지 못하고 실패하는 등 성능의 편차가 극심하게 나타났습니다.
우측 그래프를 보면 보상과 생존 스텝이 거의 완벽한 양의 선형 관계를 보입니다. 이는 현재의 보상 체계에서 생존 보상과 임무 완수 보너스의 비중이 높아 오래 버티는 것이 높은 총 보상으로 직결되는 구조임을 명확히 보여줍니다.

SAC 학습 과정 분석
SAC의 학습 과정 역시 불안정한 모습을 보였지만 DQN과 비교했을 때 진동의 폭이 상대적으로 작고 실패했을 때의 최소 보상이 에피소드 후반부로 갈수록 미세하게 상승하는 경향이 관찰됩니다.
이는 SAC가 DQN보다는 더 안정적으로 학습을 진행했음을 시사하지만, 여전히 만족스러운 수준의 수렴을 이루지는 못했습니다.
SAC 역시 DQN과 마찬가지로 보상과 생존 스텝 사이에 강한 선형 관계를 보였습니다.

5.2. 최종 모델 성능 검증
검증 결과
학습을 통해 저장된 최고의 DQN 모델을 검증 모드에서 테스트한 결과, 평균적으로 공을 10초 동안 떨어뜨리지 않고 제어하는 성능을 보였습니다.
같은 방식으로 최고의 SAC 모델을 테스트한 결과, 평균적으로 14초 동안 생존하는, DQN보다 약 40퍼센트 향상된 성능을 기록했습니다.

결과 해석
이 결과는 두 가지 중요한 사실을 알려줍니다.
비록 학습 과정은 불안정했더라도 두 알고리즘 모두 문제 해결에 필요한 유의미한 제어 정책을 학습하는 데 성공했다는 점입니다.
정교한 연속 제어가 가능한 SAC가 고정된 선택지만을 갖는 DQN에 비해 이번 과제에서 더 높은 성능을 발휘했습니다. 이는 미세한 모터 제어가 중요한 물리 제어 문제에서는 SAC와 같은 연속 행동 공간 알고리즘이 더 적합하다는 것을 실험적으로 증명한 결과입니다.


6. 결론 및 향후 과제
6.1. 결론

본 프로젝트는 불안정한 시스템 제어 문제에 대해 DQN과 SAC라는 두 가지 강화학습 알고리즘을 설계하고 적용하여 그 가능성과 한계를 명확히 확인하는 성과를 거두었습니다.
비록 학습 과정은 불안정했더라도 SAC 모델 기준 평균 14초, DQN 모델 기준 평균 10초라는 구체적이고 유의미한 생존 시간을 달성한 것입니다.
이는 사전에 프로그래밍된 제어 로직 없이 오직 데이터 기반의 시행착오만으로 복잡한 물리 제어 문제를 해결할 수 있다는 강화학습의 잠재력을 명백히 보여준 결과입니다.

연속적인 행동 공간을 직접 제어할 수 있는 SAC 알고리즘이 이산적인 행동 공간을 사용하는 DQN에 비해 이번 과제에서 더 높은 성능과 잠재력을 보여주었습니다.
이는 원탁의 미세한 기울기 조절이 중요한 물리 제어 문제의 특성상, 정해진 49개의 행동만 가능한 DQN보다 무한한 정밀도로 행동을 결정할 수 있는 SAC의 접근 방식이 더 효과적이었음을 의미합니다.

다만, 두 알고리즘 모두 학습 과정 전반에 걸쳐 성능이 안정적으로 수렴하지 못하고 심한 변동성을 보이는 명확한 한계를 보였습니다.
이는 현재의 보상 체계와 하이퍼파라미터 설정이 순간적인 최적해를 찾아내는 데는 성공했으나 지속 가능한 안정적인 정책으로 발전시키는 데에는 부족함이 있었음을 시사합니다.

6.2. 향후 과제
이번 프로젝트를 통해 얻은 결론을 바탕으로 더 높은 수준의 제어 능력을 갖춘 인공지능을 개발하기 위해 다음과 같은 향후 과제를 설정했습니다.

1. 학습 과정의 변동성을 줄이고 에이전트가 꾸준히 실력을 향상시키는 안정적인 학습 환경을 구축하는 것입니다.
긍정적 보상과 부정적 페널티의 비율을 조정하여 학습 방향을 더 세밀하게 제어할 수 있을 것입니다.
현재 100만 개로 설정된 버퍼 크기는 너무 오래된 과거의 비효율적인 경험 데이터를 과도하게 포함하여 학습을 방해했을 수 있습니다. 중요한 경험을 더 자주 샘플링하는 기법의 도입을 검토할 수 있습니다.
학습률, 할인율 감마, SAC의 엔트로피 계수 알파, 소프트 업데이트 계수 타우 등 학습 성능에 직접적인 영향을 미치는 주요 하이퍼파라미터들에 대한 체계적인 탐색 및 튜닝을 진행할 수 있습니다.

2. 신경망의 은닉층을 더 깊게 쌓거나 뉴런 수를 늘려, 더 복잡한 제어 정책을 학습할 수 있는 모델의 표현력을 강화하는 방안도 고려할 것입니다.
